\documentclass[12pt,a4paper,twoside]{article}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[margin=25mm]{geometry}
\usepackage{graphicx}
\usepackage{parskip}


\title{%
  Natural Language Processing \\
  \large Assessment 1}
\author{Matteo G. Pozzi (mgp35)}
\date{09/11/2018}

\begin{document}
\maketitle

\section{Introduction}

In this task the aim was to reproduce the paper by Pang et al. I have implemented two basic bag-of-words classifiers using Naive Bayes and SVM; initially these simply use unigrams and no extra embellishments, but I later experimented with additional functionality, as follows:

\begin{itemize}
\item Feature frequency cut-offs
\item Word stemming
\item Using bigrams as features, both exclusively or together with unigrams
\item Using feature presence as opposed to frequency
\end{itemize}

The rest of the task involved setting up the required code for 10-fold cross-validation, as well as statistical significance testing using the sign test.

\section{Method}

\subsection{Baselines}

The Naive Bayes classifier I wrote takes a very similar approach to the one I wrote for MLRD in Part IA. The algorithm runs exactly in the same way and still uses add one smoothing. My baseline SVM model uses a package, SVM\textsuperscript{light}; \footnote{https://pypi.org/project/svmlight/} I had to write code to convert the input documents into a suitable format.

In my format, each document simply consists of a list of features as strings (initially unigrams, including punctuation). SVM\textsuperscript{light} asks for a vector of features and their values (e.g. frequencies), represented as a list of such tuples, for each document. Features must be integers, so I decided to do a pre-processing step where I scan over all of the features across all documents and assign a different ID to each (I keep a map from features to IDs, and increment my ID every time I see a new feature). This gives me a bijection from features to integers, which then allows me to pass in the appropriate feature vectors.

Testing with SVM\textsuperscript{light} gives me a list of confidence values, one for each document in the test set. If this is above 0, I mark the prediction as positive, otherwise it is negative.

\subsection{(Potential!) Embellishments}

Feature frequency cut-offs are implemented by scanning through all of the training documents and creating a dictionary of feature frequencies. We then re-create the documents by copying the feature lists, but excluding features with a frequency of less than 4.

Word stemming is done by the Porter stemmer \footnote{https://tartarus.org/martin/PorterStemmer/python.txt}, which was obtained from the official homepage. \footnote{https://tartarus.org/martin/PorterStemmer/} I stemmed each review and saved the results to a separate folder.

Bigrams are generated by scanning over the documents and treating adjacent pairs of words as a feature.

Feature presence for SVM is done by making feature vectors binary - a given feature has a value of 1 iff it appears in the document (0 otherwise). The Pang et al. paper was vague regarding how to use presence with Naive Bayes, so I devised my own scheme - I still use add one smoothing, but I remove duplicates from each document's individual feature list, such that each feature in a document can only increment its frequency count once per document it appears in.


\section{Results}

The following is a table of accuracies for the Naive Bayes and SVM classifiers under differing conditions:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Conditions} & \textbf{NB} & \textbf{SVM} \\ \hline
Baseline & 81.0 & 73.5 \\ \hline
Stemming & 81.6 & 74.3 \\ \hline
Presence & 76.5 & 80.8 \\ \hline
Bigrams & 84.1 & 78.9 \\ \hline
Unigrams + Bigrams & 82.8 & 75.1 \\ \hline
Uni + Bi + Presence & 83.5 & 82.2 \\ \hline
\end{tabular}
\end{center}

One must bear in mind that the training/test data (and some implementation details) are different to Pang et al. and therefore numbers are not quite comparable. That said, however, it is encouraging to see that Naive Bayes outperforms SVM initially --- indeed, this difference is statistically significant under the sign test (at the 5\% significance level).

My hypothesis was that stemming would make an improvement --- however, the difference is not significant (for either classifier) with respect to the baseline.

Using presence instead of frequency creates a dramatic improvement in the performance of the SVM classifier --- the difference is indeed statistically significant. Naive Bayes dips a bit in performance, and this difference is significant, albeit only just (p-value is 4.3\%) --- this result is not observed by Pang et al (they notice a slight increase), although I suspect this is to do with my interpretation of feature presence in the context of Naive Bayes being potentially different to theirs.

Using bigrams as features causes Naive Bayes' performance to rise slightly, albeit not significantly --- SVM's accuracy with bigrams is actually significantly different than with unigrams. Using presence here does not cause significant differences in either case --- this is perhaps because individual bigrams have such low frequencies in each document anyway (although I would test this hypothesis if given more time).

Reintroducing unigrams together with the bigrams (but going back to frequency) causes a dip in performance for SVM (significant at the 10\% but not the 5\% level), perhaps linked to the previous poor performance with the frequency/unigrams combination. Reintroducing presence then brings this performance up to 82.2\% (which is significant), the best SVM performance out of all the experiments. Varying these same conditions for Naive Bayes produces no significant differences.

Using a feature frequency cut-off (here I chose 4) surprisingly does not produce significant differences. I tried using the cut-off under different conditions (i.e.\ rows in the table), and the results are not very interesting. The one situation that produces some sort of difference is Naive Bayes with presence --- using the cut-off in this situation yields a performance of 73.9\% (versus the 76.5\% in the table), although this difference is not significant. In any case, using feature cut-offs is a good option for improving efficiency, since it cuts down on the number of features.

\section{Conclusions}

The largest initial improvement I saw was by using presence with SVM, and as mentioned, this improvement was indeed significant. The final SVM version in the table might yield a slightly higher number, but the difference is not significant, and this more complex method is much slower than the initial improved SVM classifier with its reasonable 80.8\% accuracy. I will therefore choose the "presence" SVM classifier as my SVM baseline going forwards.

The Naive Bayes classifiers mostly show stability under differing conditions. Even comparing the versions that differ most in accuracy ("baseline" and "bigrams") does not yield a significant difference, and adding bigrams actually makes the efficiency worse. I will therefore choose the "baseline" Naive Bayes classifier as my Naive Bayes baseline going forwards --- this version represents good value for what is actually a relatively simple algorithm, and shows just how far such an algorithm can go.

\end{document}