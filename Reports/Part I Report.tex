\documentclass[12pt,a4paper,twoside]{article}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[margin=25mm]{geometry}
\usepackage{graphicx}
\usepackage{parskip}


\title{%
  Natural Language Processing \\
  \large Assessment 1}
\author{Matteo G. Pozzi (mgp35)\vspace{-2ex}}
\date{09/11/2018\vspace{-2ex}}

\begin{document}
\maketitle


\section{Introduction}

I have implemented two bag-of-words classifiers using Naive Bayes and SVM, as well experimenting with additional conditions (see section 2.2 below).

I use 10-fold cross validation throughout my experiments.


\section{Method}

\subsection{Baselines}

My baseline Naive Bayes classifier runs exactly like the one I wrote for Part IA MLRD. My baseline SVM classifier uses a package, SVM\textsuperscript{light}. \footnote{https://pypi.org/project/svmlight/}

Originally, each document simply consists of a list of tokenised features as strings. SVM\textsuperscript{light} asks for a list of \textit{(feature, value)} pairs for each document (where \textit{value} is feature frequency, initially). Features must be integers, so I scan over all of the features across all documents in sequence and assign a new ID to every new feature I see (using a dictionary).

Testing with SVM\textsuperscript{light} gives me one confidence value per test document. If this is above 0, I mark the prediction as positive, otherwise it is negative.

\subsection{(Potential!) Embellishments}

Feature frequency cut-offs are implemented by assembling dictionary of feature frequencies and then replicating each document, excluding features with a frequency less than 4.

Word stemming is done by the Porter stemmer. \footnote{Source code: https://tartarus.org/martin/PorterStemmer/python.txt}

Feature presence for SVM is done by giving a feature a value of 1 iff it appears in the document (0 otherwise). The Pang et al. paper was vague regarding presence with Naive Bayes, so I devised my own scheme - I still use add one smoothing, but I remove duplicates from each document's individual feature list.


\section{Results}

The following is a table of accuracies:

\vspace{16px}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Conditions} & \textbf{NB} & \textbf{SVM} \\ \hline
Baseline & 81.0 & 73.5 \\ \hline
Presence & 76.5 & 80.8 \\ \hline
Stemming & 81.6 & 74.3 \\ \hline
Bigrams & 84.1 & 78.9 \\ \hline
Unigrams + Bigrams & 82.8 & 75.1 \\ \hline
Uni + Bi + Presence & 83.5 & 82.2 \\ \hline
\end{tabular}
\end{center}

\vspace{16px}

Naive Bayes outperforms SVM initially, as observed by Pang et al. --- indeed, this difference is statistically significant under the sign test (at the 5\% significance level).

Using presence instead of frequency creates a significant improvement in SVM's performance. Naive Bayes worsens, albeit only just significantly (p-value is 4.3\%) --- this result differs from Pang et al, although my implementation does differ here.

My hypothesis was that stemming would make an improvement --- however, the difference is not significant (for either classifier).

Using bigrams as features is not significantly different for Naive Bayes, but is for SVM, however. Using presence here is not significant in either case --- this is \textit{perhaps} because individual bigrams have such low frequencies in each document anyway.

The last three experiments yield no significant differences for Naive Bayes. For SVM, however, introducing presence on the ``unigrams + bigrams'' model is significant, and yields the best SVM performance out of all the experiments.

Using a cut-off surprisingly does not produce significant differences, even under different conditions. The largest (albeit not significant) difference comes when using Naive Bayes with presence, which yields a performance of 73.9\% (versus the 76.5\% above).


\section{Conclusions}

The largest initial significant improvement was by using presence with SVM. The final SVM version in the table is not significantly better yet is much slower.

The Naive Bayes classifiers mostly show stability: even comparing ``baseline'' and ``bigrams'' does not yield a significant difference, and the latter is slower. The initial classifier is a great baseline --- this version represents good value for what is actually a relatively simple algorithm.

\vspace{16px}

[496 words using the TeXcount web service]


\section{Appendix}

The repository is available at https://github.com/Macro206/nlp-practicals

I have also cloned a version onto the MCS machines at the following filepath:

/home/mgp35/Desktop/NLP/nlp-practicals

This will at least include the code for the first task (I will pull changes later once the second task is finished, etc).

\end{document}